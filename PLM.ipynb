{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d256fbc0",
   "metadata": {},
   "source": [
    "# Pretrained Language Model (plm)\n",
    "\n",
    "## PLM\n",
    "\n",
    "在transformer基础上引入ELMo预训练思路，对其进行各种优化:\n",
    "\n",
    "- Google 仅选择了 Encoder 层, 通过将 Encoder 层进行堆叠，再提出不同的预训练任务-掩码语言模型（Masked Language Model，MLM），打造了一统自然语言理解（Natural Language Understanding，NLU）任务的代表模型——BERT。\n",
    "\n",
    "- OpenAI 则选择了 Decoder 层，使用原有的语言模型（Language Model，LM）任务，通过不断增加模型参数和预训练语料，打造了在 NLG（Natural Language Generation，自然语言生成）任务上优势明显的 GPT 系列模型。\n",
    "\n",
    "- 还有一种思路是同时保留 Encoder 与 Decoder，打造预训练的 Transformer 模型，例如由 Google 发布的 T5模型。\n",
    "\n",
    "## BERT （Bidirectional Encoder Representations from Transformers）\n",
    "\n",
    "预训练+微调\n",
    "\n",
    "激活函数使用的是GELU函数\n",
    "\n",
    "## \n",
    "## Decoder-Only PLM\n",
    "\n",
    "目前所有的 LLM 基本都是 Decoder-Only 模型（RWKV、Mamba 等非 Transformer 架构除外）\n",
    "\n",
    "### GPT\n",
    "\n",
    "Generative Pre-Training Language Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
